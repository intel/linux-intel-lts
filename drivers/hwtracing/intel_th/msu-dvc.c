/*
 * Intel Trace Hub to USB dvc-trace  driver
 *
 * Copyright (C) 2015, Intel Corporation.
 *
 * This software is licensed under the terms of the GNU General Public
 * License version 2, as published by the Free Software Foundation, and
 * may be copied, distributed, and modified under those terms.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */

#define pr_fmt(fmt) KBUILD_MODNAME ": %s: " fmt, __func__

#include <linux/module.h>
#include <linux/sched.h>
#include <linux/wait.h>
#include <linux/dvctrace.h>
#include <linux/usb/debug.h>
#include <linux/usb/composite.h>
#include <linux/dma-mapping.h>
#include <linux/spinlock.h>
#include <linux/list.h>
#include "msu.h"

#ifdef MDD_DEBUG
#define MDD_F_DEBUG() pr_debug("\n")
#else
#define MDD_F_DEBUG() do {} while (0)
#endif

#define DTC_DRV_NAME "dvcith"

#define MDD_MIN_TRANSFER_DEF	2048
#define MDD_RETRY_TIMEOUT_DEF	2
#define MDD_MAX_RETRY_CNT_DEF	150

/* The DWC3 gadget is able to handle a maximum of 32 TRBs per-ep (an sg based
 * request counts as the number of sg-s).
 * This should be updated in case some other UDC has a lower threshold. */
#define MDD_MAX_TRB_CNT		32

#define mdd_err(mdd, ...) dev_err(&(mdd)->ddev.device, ## __VA_ARGS__)
#define mdd_warn(mdd, ...) dev_warn(&(mdd)->ddev.device, ## __VA_ARGS__)
#define mdd_info(mdd, ...) dev_info(&(mdd)->ddev.device, ## __VA_ARGS__)
#define mdd_debug(mdd, ...) dev_debug(&(mdd)->ddev.device, ## __VA_ARGS__)

#ifdef MDD_DEBUG
struct msu_dvc_stats {
	unsigned long work_start;
	unsigned long work_end;

	unsigned long loop_count;
	unsigned long hits;

	u64 full_block_size;
	u64 valid_block_size;
	u64 valid_data_size;

	u32 transfer_type:2;
	u32 process_type:2;

	enum usb_device_speed speed;
};
#endif

enum {
	MDD_TRANSFER_NO_CHANGE,
	MDD_TRANSFER_AUTO,
	MDD_TRANSFER_MIN = MDD_TRANSFER_AUTO,
	MDD_TRANSFER_SINGLE,
	MDD_TRANSFER_MULTI,
	MDD_TRANSFER_MAX = MDD_TRANSFER_MULTI,
};

static const char *const transfer_type_name[] = {
	[MDD_TRANSFER_AUTO] = "Auto",
	[MDD_TRANSFER_SINGLE] = "Single",
	[MDD_TRANSFER_MULTI] = "Multi",
};

enum {
	MDD_PROC_NO_CHANGE,
	MDD_PROC_NONE,
	MDD_PROC_MIN = MDD_PROC_NONE,
	MDD_PROC_REM_TAIL,
	MDD_PROC_REM_ALL,
	MDD_PROC_MAX = MDD_PROC_REM_ALL,
};

static const char *const process_type_name[] = {
	[MDD_PROC_NONE] = "Full-Blocks",
	[MDD_PROC_REM_TAIL] = "Trimmed-Blocks",
	[MDD_PROC_REM_ALL] = "STP",
};

struct mdd_transfer_data {
	u8 *buffer;
	u8 *buffer_sg;
	size_t buffer_sg_len;
	dma_addr_t buffer_dma;
	size_t buffer_len;
	struct scatterlist *sg_raw;
	struct scatterlist *sg_proc;
	struct scatterlist *sg_trans;	/* not be allocated */
	size_t block_count;
	size_t block_size;
	spinlock_t lock;
};

#define mdd_lock_transfer(mdd) spin_lock(&mdd->tdata.lock)
#define mdd_unlock_transfer(mdd) spin_unlock(&mdd->tdata.lock)

struct msu_dvc_dev {
	struct dvct_source_device ddev;
	atomic_t *dtc_status;
	struct usb_ep *ep;
	struct usb_function *func;
	enum usb_device_speed speed;
	struct intel_th_device *th_dev;

	struct workqueue_struct *wrq;
	struct work_struct work;
	struct usb_request *req;
	wait_queue_head_t wq;
	atomic_t req_ongoing;

	/*attributes */
	u32 retry_timeout;
	u32 max_retry_count;
	u32 transfer_type:2;
	u32 process_type:2;
	u32 min_transfer;

#ifdef MDD_DEBUG
	struct msu_dvc_stats stats;
#endif
	struct mdd_transfer_data tdata;

	struct list_head mdd_list;
};

static LIST_HEAD(mdd_devs);
static DEFINE_SPINLOCK(mdd_devs_lock);

static inline struct usb_gadget *mdd_gadget(struct msu_dvc_dev *mdd)
{
	BUG_ON(!mdd->func);
	BUG_ON(!mdd->func->config);
	BUG_ON(!mdd->func->config->cdev);
	BUG_ON(!mdd->func->config->cdev->gadget);
	return mdd->func->config->cdev->gadget;
}

/* Back-cast to msu_dvc_dev */
static inline struct msu_dvc_dev *dtc_to_mdd(struct dvct_source_device *p_dtc)
{
	return container_of(p_dtc, struct msu_dvc_dev, ddev);
}

static ssize_t mdd_min_transfer_show(struct device *dev,
				      struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	return sprintf(buf, "%u\n", mdd->min_transfer);
}

static ssize_t mdd_min_transfer_store(struct device *dev,
				       struct device_attribute *attr,
				       const char *buf, size_t count)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);

	if (mdd->dtc_status
	    && dvct_get_status(mdd->dtc_status, DVCT_MASK_TRANS))
		return -EBUSY;

	/* 48 represents the size of sync frames that are generated by
	 * a window switch, from this point on we have "real data"
	 * Going under this value could result in unneeded switching */
	if (!kstrtou32(buf, 10, &mdd->min_transfer)) {
		if (mdd->min_transfer < 48)
			mdd->min_transfer = 48;
		return count;
	}

	return -EINVAL;
}

static DEVICE_ATTR_RW(mdd_min_transfer);


static ssize_t mdd_retry_timeout_show(struct device *dev,
				      struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	return sprintf(buf, "%u\n", mdd->retry_timeout);
}

static ssize_t mdd_retry_timeout_store(struct device *dev,
				       struct device_attribute *attr,
				       const char *buf, size_t count)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	if (!kstrtou32(buf, 10, &mdd->retry_timeout))
		return count;

	return -EINVAL;
}

static DEVICE_ATTR_RW(mdd_retry_timeout);

static ssize_t mdd_max_retry_show(struct device *dev,
				  struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	return sprintf(buf, "%u\n", mdd->max_retry_count);
}

static ssize_t mdd_max_retry_store(struct device *dev,
				   struct device_attribute *attr,
				   const char *buf, size_t count)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	if (!kstrtou32(buf, 10, &mdd->max_retry_count))
		return count;

	return -EINVAL;
}

static DEVICE_ATTR_RW(mdd_max_retry);

static ssize_t mdd_transfer_type_show(struct device *dev,
				      struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	return sprintf(buf, "%d %s\n", mdd->transfer_type,
		       transfer_type_name[mdd->transfer_type]);
}

static ssize_t mdd_transfer_type_store(struct device *dev,
				       struct device_attribute *attr,
				       const char *buf, size_t count)
{
	struct msu_dvc_dev *mdd;
	u8 tmp;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);

	if (mdd->dtc_status
	    && dvct_get_status(mdd->dtc_status, DVCT_MASK_TRANS))
		return -EBUSY;

	if (!kstrtou8(buf, 10, &tmp) && tmp <= MDD_TRANSFER_MAX
	    && tmp >= MDD_TRANSFER_MIN) {
		mdd->transfer_type = tmp;
		return count;
	}
	return -EINVAL;
}

static DEVICE_ATTR_RW(mdd_transfer_type);

static ssize_t mdd_proc_type_show(struct device *dev,
				  struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);
	return sprintf(buf, "%d %s\n", mdd->process_type,
		       process_type_name[mdd->process_type]);
}

static ssize_t mdd_proc_type_store(struct device *dev,
				   struct device_attribute *attr,
				   const char *buf, size_t count)
{
	struct msu_dvc_dev *mdd;
	u8 tmp;

	MDD_F_DEBUG();

	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);

	if (mdd->dtc_status
	    && dvct_get_status(mdd->dtc_status, DVCT_MASK_TRANS))
		return -EBUSY;

	if (!kstrtou8(buf, 10, &tmp) && tmp <= MDD_PROC_MAX
	    && tmp >= MDD_PROC_MIN) {
		mdd->process_type = tmp;
		return count;
	}
	return -EINVAL;
}

static DEVICE_ATTR_RW(mdd_proc_type);

#ifdef MDD_DEBUG

static ssize_t mdd_stats_show(struct device *dev,
			      struct device_attribute *attr, char *buf)
{
	struct msu_dvc_dev *mdd;
	int len = 0;

	static const char *const u_speed_names[] = {
		[USB_SPEED_UNKNOWN] = "?",
		[USB_SPEED_LOW] = "LS",
		[USB_SPEED_FULL] = "FS",
		[USB_SPEED_HIGH] = "HS",
		[USB_SPEED_WIRELESS] = "WR",
		[USB_SPEED_SUPER] = "SS",
	};

	MDD_F_DEBUG();
	mdd = container_of(dev, struct msu_dvc_dev, ddev.device);

	len += snprintf(buf + len, PAGE_SIZE - len, "R.count\tR.hits\t");

	len += snprintf(buf + len, PAGE_SIZE - len, "T.tot_j\tT.tot_ms\t");
	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "D.total\tD.block\tT.stp\t");
	len += snprintf(buf + len, PAGE_SIZE - len, "Tr.type\tProc.type\t");

	len += snprintf(buf + len, PAGE_SIZE - len, "USB.speed\n");

	/* Actual values starts here */
	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%lu\t%lu\t",
		     mdd->stats.loop_count, mdd->stats.hits);

	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%lu\t%u\t",
		     (mdd->stats.work_end - mdd->stats.work_start),
		     jiffies_to_msecs(mdd->stats.work_end -
				      mdd->stats.work_start));
	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%llu\t%llu\t%llu\t",
		     mdd->stats.full_block_size, mdd->stats.valid_block_size,
		     mdd->stats.valid_data_size);
	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%s\t",
		     transfer_type_name[mdd->transfer_type]);
	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%s\t",
		     process_type_name[mdd->process_type]);

	len +=
	    snprintf(buf + len, PAGE_SIZE - len, "%s\n",
		     u_speed_names[mdd->stats.speed]);

	return len;
}

static DEVICE_ATTR_RO(mdd_stats);

static void init_stats_start(struct msu_dvc_dev *mdd)
{
	mdd->stats.loop_count = 0;
	mdd->stats.hits = 0;

	mdd->stats.work_start = jiffies;

	mdd->stats.full_block_size = 0;
	mdd->stats.valid_block_size = 0;
	mdd->stats.valid_data_size = 0;

	mdd->stats.process_type = mdd->process_type;
	mdd->stats.transfer_type = mdd->transfer_type;
	mdd->stats.speed = mdd->speed;
}

#define stats_loop(mdd) ((mdd)->stats.loop_count++)
#define stats_hit(mdd) ((mdd)->stats.hits++)
#else
#define init_stats_start(n) do {} while (0)
#define stats_loop(mdd) do {} while (0)
#define stats_hit(mdd) do {} while (0)
#endif

static void mdd_complete(struct usb_ep *ep, struct usb_request *req)
{
	struct msu_dvc_dev *mdd = (struct msu_dvc_dev *)req->context;

	mdd_lock_transfer(mdd);

	if (req->status != 0) {
		mdd_err(mdd, "Usb request error %d\n", req->status);
		dvct_clr_status(mdd->dtc_status, DVCT_MASK_TRANS);
		dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
	}
	atomic_set(&mdd->req_ongoing, 0);
	wake_up(&mdd->wq);
	mdd_unlock_transfer(mdd);
}

static int mdd_setup_transfer_data(struct msu_dvc_dev *mdd)
{
	int ret = -EINVAL;

	MDD_F_DEBUG();

	if (!mdd->ep || !mdd->req) {
		mdd_err(mdd, "Invalid endpoint data\n");
		goto err;
	}

	mdd->tdata.block_count = msc_max_blocks(mdd->th_dev);
	if (mdd->tdata.block_count == 0) {
		mdd_err(mdd, "Invalid block count %zu\n",
			mdd->tdata.block_count);
		goto err;
	}

	mdd->tdata.block_size = msc_block_max_size(mdd->th_dev);
	if (mdd->tdata.block_size == 0) {
		mdd_err(mdd, "Invalid block size %zu\n", mdd->tdata.block_size);
		goto err;
	}

	mdd->tdata.sg_raw = kmalloc_array(mdd->tdata.block_count,
					  sizeof(*mdd->tdata.sg_raw),
					  GFP_KERNEL);
	if (!mdd->tdata.sg_raw) {
		mdd_err(mdd, "Cannot allocate sg memory %zu\n",
			mdd->tdata.block_size);
		goto err_sg_raw;
	}

	if (mdd->process_type != MDD_PROC_NONE) {
		mdd->tdata.sg_proc = kmalloc_array(mdd->tdata.block_count,
						   sizeof(*mdd->tdata.sg_proc),
						   GFP_KERNEL);
		if (!mdd->tdata.sg_proc) {
			mdd_err(mdd, "Cannot allocate sg memory %zu\n",
				mdd->tdata.block_size);
			goto err_sg_proc;
		}
		mdd->tdata.sg_trans = mdd->tdata.sg_proc;
	} else {
		mdd->tdata.sg_trans = mdd->tdata.sg_raw;
	}

	if (mdd->transfer_type == MDD_TRANSFER_SINGLE) {
		mdd->tdata.buffer_len =
		    mdd->tdata.block_count * mdd->tdata.block_size;
		mdd->tdata.buffer =
		    dma_alloc_coherent(&(mdd_gadget(mdd)->dev),
				       mdd->tdata.buffer_len,
				       &mdd->tdata.buffer_dma, GFP_KERNEL);
		if (!mdd->tdata.buffer) {
			mdd_err(mdd, "Cannot allocate DMA memory\n");
			goto err_l_buf;
		}
	} else {
		mdd->tdata.buffer_sg_len =
		    mdd->tdata.block_count * mdd->tdata.block_size;
		mdd->tdata.buffer_sg = kmalloc(mdd->tdata.buffer_sg_len, GFP_KERNEL);
		if(mdd->tdata.buffer_sg == NULL)
			mdd->tdata.buffer_sg_len = 0;

		mdd->tdata.buffer = NULL;
		mdd->tdata.buffer_dma = 0;
		mdd->tdata.buffer_len = 0;
	}
	return 0;
err_l_buf:
	kfree(mdd->tdata.sg_proc);
	mdd->tdata.sg_proc = NULL;
err_sg_proc:
	kfree(mdd->tdata.sg_raw);
	mdd->tdata.sg_raw = NULL;
err_sg_raw:
	ret = -ENOMEM;
err:
	return ret;
}

static void mdd_reset_transfer_data(struct msu_dvc_dev *mdd)
{
	MDD_F_DEBUG();
	kfree(mdd->tdata.sg_proc);
	mdd->tdata.sg_proc = NULL;
	kfree(mdd->tdata.sg_raw);
	mdd->tdata.sg_raw = NULL;
	if (mdd->tdata.buffer) {
		dma_free_coherent(&(mdd_gadget(mdd)->dev),
				  mdd->tdata.buffer_len, mdd->tdata.buffer,
				  mdd->tdata.buffer_dma);
		mdd->tdata.buffer = NULL;
		mdd->tdata.buffer_dma = 0;
		mdd->tdata.buffer_len = 0;
	}

	if(mdd->tdata.buffer_sg != NULL)
	{
		mdd->tdata.buffer_sg_len = 0;
		kfree(mdd->tdata.buffer_sg);
	}
}

static unsigned mdd_sg_len(struct scatterlist *sgl, int nents)
{
	int i;
	struct scatterlist *sg;
	unsigned ret = 0;

	/*MDD_F_DEBUG(); */
	for_each_sg(sgl, sg, nents, i) {
		ret += sg->length;
	}
	return ret;
}

static int mdd_send_sg_buffer(struct msu_dvc_dev *mdd, int nents)
{
	size_t transfer_len;

	/*MDD_F_DEBUG(); */
	mdd_lock_transfer(mdd);
	transfer_len =
	    sg_copy_to_buffer(mdd->tdata.sg_trans, nents, mdd->tdata.buffer_sg,
			      mdd->tdata.buffer_sg_len);

	if (!transfer_len) {
		mdd_err(mdd, "Cannot copy into nonsg memory\n");
		mdd_unlock_transfer(mdd);
		return -EINVAL;
	}

	mdd->req->buf = mdd->tdata.buffer_sg;
	mdd->req->length = transfer_len;
	mdd->req->dma = 0;
	mdd->req->sg = NULL;
	mdd->req->num_sgs = 0;

	mdd->req->context = mdd;
	mdd->req->complete = mdd_complete;
	mdd->req->zero = 1;

	if (usb_ep_queue(mdd->ep, mdd->req, GFP_KERNEL)) {
		mdd_err(mdd, "Cannot queue request\n");
		dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
		mdd_unlock_transfer(mdd);
		return -EINVAL;
	}

	atomic_set(&mdd->req_ongoing, 1);
	mdd_unlock_transfer(mdd);
	/*wait for done stop or disable */
	wait_event(mdd->wq, (!atomic_read(&mdd->req_ongoing) ||
			     (atomic_read(mdd->dtc_status) !=
			      DVCT_MASK_ONLINE_TRANS)));
	return 0;
}

static int mdd_send_sg(struct msu_dvc_dev *mdd, int nents)
{
	struct scatterlist *sgl = mdd->tdata.sg_trans;

	if(mdd->tdata.buffer_sg != NULL)
	{
		return mdd_send_sg_buffer(mdd, nents);
	}

	/*MDD_F_DEBUG(); */
	while (nents) {
		int trans_ents;

		mdd_lock_transfer(mdd);

		if (nents > MDD_MAX_TRB_CNT) {
			trans_ents = MDD_MAX_TRB_CNT;
			sg_mark_end(&sgl[trans_ents - 1]);
		} else {
			trans_ents = nents;
		}

		if (trans_ents == 1) {
			mdd->req->buf = sg_virt(sgl);
			mdd->req->length = sgl->length;
			mdd->req->dma = 0;
			mdd->req->sg = NULL;
			mdd->req->num_sgs = 0;
		} else {
			mdd->req->buf = NULL;
			mdd->req->length = mdd_sg_len(sgl, trans_ents);
			mdd->req->dma = 0;
			mdd->req->sg = sgl;
			mdd->req->num_sgs = trans_ents;
		}

		mdd->req->context = mdd;
		mdd->req->complete = mdd_complete;
		mdd->req->zero = 1;

		if (usb_ep_queue(mdd->ep, mdd->req, GFP_KERNEL)) {
			mdd_err(mdd, "Cannot queue request\n");
			dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
			mdd_unlock_transfer(mdd);
			return -EINVAL;
		}

		atomic_set(&mdd->req_ongoing, 1);
		nents -= trans_ents;
		sgl += trans_ents;

		mdd_unlock_transfer(mdd);
		/*wait for done stop or disable */
		wait_event(mdd->wq, (!atomic_read(&mdd->req_ongoing) ||
				     (atomic_read(mdd->dtc_status) !=
				      DVCT_MASK_ONLINE_TRANS)));
	}
	return 0;
}

static int mdd_send_buffer(struct msu_dvc_dev *mdd, int nents)
{
	size_t transfer_len;

	/*MDD_F_DEBUG(); */
	mdd_lock_transfer(mdd);
	transfer_len =
	    sg_copy_to_buffer(mdd->tdata.sg_trans, nents, mdd->tdata.buffer,
			      mdd->tdata.buffer_len);
	if (!transfer_len) {
		mdd_err(mdd, "Cannot copy into nonsg memory\n");
		mdd_unlock_transfer(mdd);
		return -EINVAL;
	}
	mdd->req->buf = mdd->tdata.buffer;
	mdd->req->length = transfer_len;
	mdd->req->dma = mdd->tdata.buffer_dma;
	mdd->req->sg = NULL;
	mdd->req->num_sgs = 0;

	mdd->req->context = mdd;
	mdd->req->complete = mdd_complete;
	mdd->req->zero = 1;

	if (usb_ep_queue(mdd->ep, mdd->req, GFP_KERNEL)) {
		mdd_err(mdd, "Cannot queue request\n");
		dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
		mdd_unlock_transfer(mdd);
		return -EINVAL;
	}

	atomic_set(&mdd->req_ongoing, 1);
	mdd_unlock_transfer(mdd);
	/*wait for done stop or disable */
	wait_event(mdd->wq, (!atomic_read(&mdd->req_ongoing) ||
			     (atomic_read(mdd->dtc_status) !=
			      DVCT_MASK_ONLINE_TRANS)));
	return 0;
}

static int mdd_send_auto(struct msu_dvc_dev *mdd, int nents)
{
	/*MDD_F_DEBUG(); */
	if (!mdd_gadget(mdd)->sg_supported)
		return mdd_send_buffer(mdd, nents);
	else
		return mdd_send_sg(mdd, nents);
}

static int (*send_funcs[])(struct msu_dvc_dev *, int) = {
	[MDD_TRANSFER_AUTO] = mdd_send_auto,
	[MDD_TRANSFER_SINGLE] = mdd_send_buffer,
	[MDD_TRANSFER_MULTI] = mdd_send_sg,
};

#ifdef MDD_DEBUG
static int mdd_proc_add_stats(struct msu_dvc_dev *mdd, int nents)
{
	int i, count;
	struct scatterlist *sg;

	/*MDD_F_DEBUG(); */
	for_each_sg(mdd->tdata.sg_raw, sg, nents, i) {
		count = msc_data_sz((struct msc_block_desc *)sg_virt(sg));
		mdd->stats.full_block_size += sg->length;
		mdd->stats.valid_block_size += (count + MSC_BDESC);
		mdd->stats.valid_data_size += count;
	}

	return i;
}
#else
#define mdd_proc_add_stats(m, n) do {} while (0)
#endif

static int mdd_proc_trimmed_blocks(struct msu_dvc_dev *mdd, int nents)
{
	u8 *ptr;
	size_t len;
	int i, out_cnt = 0;
	struct scatterlist *sg, *sg_dest = NULL;

	/*MDD_F_DEBUG(); */
	mdd_proc_add_stats(mdd, nents);

	sg_init_table(mdd->tdata.sg_proc, nents);

	for_each_sg(mdd->tdata.sg_raw, sg, nents, i) {
		ptr = sg_virt(sg);
		len = msc_data_sz((struct msc_block_desc *)ptr);
		if (!len) {
			mdd_err(mdd, "Zero length block");
			continue;
		}

		len += MSC_BDESC;

		if (!sg_dest)
			sg_dest = mdd->tdata.sg_proc;
		else
			sg_dest = sg_next(sg_dest);
		sg_set_buf(sg_dest, ptr, len);
		out_cnt++;
	}
	if (sg_dest)
		sg_mark_end(sg_dest);

	return out_cnt;
}

static int mdd_proc_stp_only(struct msu_dvc_dev *mdd, int nents)
{
	u8 *ptr;
	size_t len;
	int i, out_cnt = 0;
	struct scatterlist *sg, *sg_dest = NULL;

	/*MDD_F_DEBUG(); */
	mdd_proc_add_stats(mdd, nents);

	sg_init_table(mdd->tdata.sg_proc, nents);

	for_each_sg(mdd->tdata.sg_raw, sg, nents, i) {
		ptr = sg_virt(sg);
		len = msc_data_sz((struct msc_block_desc *)ptr);
		ptr += MSC_BDESC;
		if (!len) {
			mdd_err(mdd, "Zero data length block");
		} else {
			if (!sg_dest)
				sg_dest = mdd->tdata.sg_proc;
			else
				sg_dest = sg_next(sg_dest);
			sg_set_buf(sg_dest, ptr, len);
			out_cnt++;
		}
	}
	if (sg_dest)
		sg_mark_end(sg_dest);

	return out_cnt;
}

static int (*proc_funcs[]) (struct msu_dvc_dev *, int) = {
#ifdef MDD_DEBUG
	[MDD_PROC_NONE] = mdd_proc_add_stats,
#endif
	[MDD_PROC_REM_TAIL] = mdd_proc_trimmed_blocks,
	[MDD_PROC_REM_ALL] = mdd_proc_stp_only,
};

static void mdd_work(struct work_struct *work)
{
	int nents, current_bytes, retry_cnt = 0;
	struct msu_dvc_dev *mdd;

	MDD_F_DEBUG();
	mdd = container_of(work, struct msu_dvc_dev, work);
	init_stats_start(mdd);

	if (mdd_setup_transfer_data(mdd)) {
		mdd_err(mdd, "Cannot setup transfer data\n");
		return;
	}
	mdd_info(mdd, "Start transfer loop\n");
	while (atomic_read(mdd->dtc_status) == DVCT_MASK_ONLINE_TRANS) {
		sg_init_table(mdd->tdata.sg_raw, mdd->tdata.block_count);
		/* Maybe will be better if msc_sg_oldest_win changes the window
		 * if the "oldest" one contains data and is the current one..*/

		preempt_disable();
		current_bytes = msc_current_win_bytes(mdd->th_dev);
		if (current_bytes > mdd->min_transfer ||
		    (current_bytes && retry_cnt >= mdd->max_retry_count)) {
			msc_switch_window(mdd->th_dev);
			nents = msc_sg_oldest_win(mdd->th_dev,
						  mdd->tdata.sg_raw);
			retry_cnt = 0;
		} else {
			if (unlikely(current_bytes < 0)) {
				mdd_warn(mdd, "Unexpected state (%d), switch",
					 current_bytes);
				msc_switch_window(mdd->th_dev);
			} else {
				if (retry_cnt < mdd->max_retry_count)
					retry_cnt++;
			}
			nents = 0;
		}
		preempt_enable();
		stats_loop(mdd);

		if (nents < 0) {
			mdd_err(mdd, "Cannot get ith data\n");
			dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
			break;
		}

		if (nents && proc_funcs[mdd->process_type]) {
			nents = proc_funcs[mdd->process_type] (mdd, nents);
			if (nents < 0) {
				mdd_err(mdd, "Cannot process data\n");
				dvct_set_status(mdd->dtc_status, DVCT_MASK_ERR);
				break;
			}
		}

		if (nents) {
			stats_hit(mdd);
			if (send_funcs[mdd->transfer_type] (mdd, nents))
				break;
		} else {
			/*wait for stop or timeout */
			wait_event_timeout(mdd->wq,
					   (atomic_read(mdd->dtc_status) !=
					    DVCT_MASK_ONLINE_TRANS),
					   msecs_to_jiffies(mdd->
							    retry_timeout));
		}
	}
	mdd_info(mdd, "End transfer loop\n");
	if (atomic_read(&mdd->req_ongoing)) {
		usb_ep_dequeue(mdd->ep, mdd->req);
		atomic_set(&mdd->req_ongoing, 0);
	}

#ifdef MDD_DEBUG
	mdd->stats.work_end = jiffies;
#endif
	mdd_reset_transfer_data(mdd);
}

static int mdd_activate(struct dvct_source_device *client, atomic_t *status)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();

	mdd->dtc_status = status;

	return 0;
}

static int mdd_binded(struct dvct_source_device *client, struct usb_ep *ep,
		      struct usb_function *func)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();
	mdd->ep = ep;
	mdd->func = func;

	mdd->req = usb_ep_alloc_request(mdd->ep, GFP_KERNEL);
	if (!mdd->req) {
		mdd_err(mdd, "Cannot allocate usb request\n");
		return -ENOMEM;
	}
	return 0;
}

static void mdd_connected(struct dvct_source_device *client,
			  enum usb_device_speed speed)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();
	mdd->speed = speed;
}

union mdd_config {
	u8 config;
	struct {
		u8 enable:1;	/* one */
		u8 tr_type:2;
		u8 proc_type:2;
	} params;
};

static int mdd_start_transfer(struct dvct_source_device *client, u8 config)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);
	union mdd_config cfg;

	MDD_F_DEBUG();
	/* If we share the resources with node base reading this is
	 * the place where we should lock.*/

	cfg.config = config;

	if (cfg.params.proc_type <= MDD_PROC_MAX
	    && cfg.params.proc_type >= MDD_PROC_MIN) {
		mdd_info(mdd, "Set process type %d", cfg.params.proc_type);
		mdd->process_type = cfg.params.proc_type;
	}

	if (cfg.params.tr_type <= MDD_TRANSFER_MAX
	    && cfg.params.tr_type >= MDD_TRANSFER_MIN) {
		mdd_info(mdd, "Set transfer type %d", cfg.params.tr_type);
		mdd->transfer_type = cfg.params.tr_type;
	}

	/*Force linear buffer transfer if the gadget is not supporting SGs */
	if (mdd->transfer_type != MDD_TRANSFER_SINGLE
	    && !mdd_gadget(mdd)->sg_supported) {
		mdd_info(mdd, "Force linear buffer transfer");
		mdd->transfer_type = MDD_TRANSFER_SINGLE;
	}

	dvct_clr_status(mdd->dtc_status, DVCT_MASK_ERR);
	dvct_set_status(mdd->dtc_status, DVCT_MASK_TRANS);
	queue_work(mdd->wrq, &mdd->work);
	return 0;
}

static int mdd_stop_transfer(struct dvct_source_device *client)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();
	dvct_clr_status(mdd->dtc_status, DVCT_MASK_TRANS);
	wake_up(&mdd->wq);

	return 0;
}

static void mdd_disconnected(struct dvct_source_device *client)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();
	mdd->speed = USB_SPEED_UNKNOWN;
}

static void mdd_unbinded(struct dvct_source_device *client)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();

	if (mdd->req) {
		usb_ep_free_request(mdd->ep, mdd->req);
		mdd->req = NULL;
	}
	mdd->ep = NULL;
}

static void mdd_deactivate(struct dvct_source_device *client)
{
	struct msu_dvc_dev *mdd = dtc_to_mdd(client);

	MDD_F_DEBUG();
	mdd->dtc_status = NULL;
}

/*the driver*/
static struct dvct_source_driver mdd_drv = {
	.activate = mdd_activate,
	.binded = mdd_binded,
	.connected = mdd_connected,
	.start_transfer = mdd_start_transfer,
	.stop_transfer = mdd_stop_transfer,
	.disconnected = mdd_disconnected,
	.unbinded = mdd_unbinded,
	.deactivate = mdd_deactivate,
	.driver.name = DTC_DRV_NAME,
};

static struct msu_dvc_dev *mdd_alloc_device(const char *name)
{
	struct msu_dvc_dev *mdd;

	mdd = kzalloc(sizeof(*mdd), GFP_KERNEL);

	if (!mdd)
		return ERR_PTR(-ENOMEM);

	mdd->ddev.name_add = kstrdup(name, GFP_KERNEL);
	if (!mdd->ddev.name_add) {
		kfree(mdd);
		return ERR_PTR(-ENOMEM);
	}

	/* mdd->ddev.protocol = 0; */
	/* mdd->ddev.desc = NULL; */
	/* mdd->dtc_status = NULL; */
	/* mdd->ep = NULL; */
	mdd->speed = USB_SPEED_UNKNOWN;
	/* mdd->msu_dev = NULL; */
	/* mdd->wrq = NULL; */
	mdd->retry_timeout = MDD_RETRY_TIMEOUT_DEF;
	mdd->max_retry_count = MDD_MAX_RETRY_CNT_DEF;
	/* mdd->req = NULL; */
	atomic_set(&mdd->req_ongoing, 0);
	/*mdd->tdata is all NULL */
	mdd->transfer_type = MDD_TRANSFER_AUTO;
	mdd->process_type = MDD_PROC_REM_ALL;
	mdd->min_transfer = MDD_MIN_TRANSFER_DEF;

	spin_lock_init(&mdd->tdata.lock);

	return mdd;
};

static void mdd_free_device(struct msu_dvc_dev *mdd)
{
	kfree(mdd->ddev.name_add);
	kfree(mdd);
};

static struct attribute *mdd_attrs[] = {
	&dev_attr_mdd_min_transfer.attr,
	&dev_attr_mdd_retry_timeout.attr,
	&dev_attr_mdd_max_retry.attr,
	&dev_attr_mdd_transfer_type.attr,
	&dev_attr_mdd_proc_type.attr,
#ifdef MDD_DEBUG
	&dev_attr_mdd_stats.attr,
#endif
	NULL,
};

static struct attribute_group mdd_attrs_group = {
	.attrs	= mdd_attrs,
};

void mdd_msc_probe(struct intel_th_device *thdev)
{
	int ret;
	struct msu_dvc_dev *mdd;
	struct device *dev;

	pr_info("New th-msc device %s", dev_name(&thdev->dev));
	mdd = mdd_alloc_device(dev_name(&thdev->dev));

	if (IS_ERR_OR_NULL(mdd)) {
		pr_err("Cannot allocate device %s (%ld)", dev_name(&thdev->dev),
		       PTR_ERR(mdd));
		return;
	}

	ret = dvct_source_device_add(&mdd->ddev, &mdd_drv);
	if (ret) {
		pr_err("Cannot register dvc device %d", ret);
		mdd_free_device(mdd);
		return;
	}

	mdd->th_dev = thdev;
	dev = &mdd->ddev.device;

	mdd->wrq = alloc_workqueue("%s_workqueue", WQ_MEM_RECLAIM | WQ_HIGHPRI,
				   1, dev_name(&mdd->ddev.device));
	if (!mdd->wrq) {
		mdd_err(mdd, "Cannot allocate work queue\n");
		mdd_free_device(mdd);
		return;
	}

	INIT_WORK(&mdd->work, mdd_work);

	init_waitqueue_head(&mdd->wq);

	/*Attributes */
	ret = sysfs_create_group(&dev->kobj, &mdd_attrs_group);
	if (ret)
		mdd_warn(mdd, "Cannot add attribute group %d\n", ret);

	ret = sysfs_create_link(&dev->kobj, &thdev->dev.kobj, "msc");
	if (ret)
		mdd_warn(mdd, "Cannot add msc link %d\n", ret);

	spin_lock(&mdd_devs_lock);
	list_add(&mdd->mdd_list, &mdd_devs);
	spin_unlock(&mdd_devs_lock);
}

void mdd_msc_remove(struct intel_th_device *thdev)
{
	struct msu_dvc_dev *mdd = NULL;
	struct msu_dvc_dev *mdd_iter = NULL;

	spin_lock(&mdd_devs_lock);
	list_for_each_entry(mdd_iter, &mdd_devs, mdd_list) {
		if (mdd_iter->th_dev == thdev)
			mdd = mdd_iter;
	}

	if (!mdd) {
		pr_err("No such mdd device, %s", dev_name(&thdev->dev));
		spin_unlock(&mdd_devs_lock);
		return;
	}
	list_del(&mdd->mdd_list);

	spin_unlock(&mdd_devs_lock);

	flush_workqueue(mdd->wrq);
	destroy_workqueue(mdd->wrq);

	sysfs_remove_group(&mdd->ddev.device.kobj, &mdd_attrs_group);
	sysfs_remove_link(&mdd->ddev.device.kobj, "msc");

	mdd->wrq = NULL;

	dvct_source_device_del(&mdd->ddev);
	mdd_free_device(mdd);
}

struct msc_probe_rem_cb mdd_msc_cbs = {
	.probe = mdd_msc_probe,
	.remove = mdd_msc_remove,
};

static int __init msu_dvc_init(void)
{
	int ret;

	MDD_F_DEBUG();
	ret = dvct_source_driver_register(&mdd_drv);
	if (ret) {
		pr_err("Cannot register dvc driver %d", ret);
		return ret;
	}

	msc_register_callbacks(mdd_msc_cbs);
	return 0;
}

static void __exit msu_dvc_exit(void)
{
	MDD_F_DEBUG();
	msc_unregister_callbacks();
	dvct_source_driver_unregister(&mdd_drv);
}

module_init(msu_dvc_init);
module_exit(msu_dvc_exit);

MODULE_LICENSE("GPL v2");
MODULE_AUTHOR("Traian Schiau <traianx.schiau@intel.com>");
